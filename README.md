---

# AI Agent Chatbot (LangChain Â· Groq Â· OpenAI Â· Tavily)

An end-to-end **AI Agent Chatbot** featuring a FastAPI backend and Streamlit frontend.
It supports multiple LLM providers, optional web search, and a clean, modular agent architecture.

---

## Overview

This project demonstrates how to build a modern AI agent using **LangChain**, with seamless switching between **Groq** and **OpenAI** models, optional **real-time web search**, and a user-friendly web interface.

---

## Features

* Switch between **Groq** and **OpenAI** providers
* Optional real-time web search via **Tavily**
* Custom system prompts
* Fast inference with **Groq**
* Modular, extensible agent design
* Interactive **Streamlit** UI
* Safety checks for model/provider compatibility

---

## Project Structure

```text
AI Agent Chatbot/
â”‚
â”œâ”€â”€ backend.py          # FastAPI server
â”œâ”€â”€ ai_agent.py         # LangChain agent logic
â”œâ”€â”€ frontend.py         # Streamlit UI
â”œâ”€â”€ .env                # API keys (not committed)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Environment Setup

### 1ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv myenv
source myenv/bin/activate   # macOS / Linux
# myenv\Scripts\activate    # Windows
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## Environment Variables

Create a `.env` file in the project root:

```env
GROQ_API_KEY=gsk_xxxxxxxxxxxxx
OPENAI_API_KEY=sk_xxxxxxxxxxxxx
TAVILY_API_KEY=tvly_xxxxxxxxxxx
```

> âš ï¸ **Note:** OpenAI requires an active billing quota.
> If your OpenAI quota is exhausted, use **Groq** instead.

---

## ğŸš€ Running the Application

### 1ï¸âƒ£ Start the Backend (FastAPI)

```bash
uvicorn backend:app --host 127.0.0.1 --port 9999
```

Verify the server is running:

```text
Uvicorn running on http://127.0.0.1:9999
```

---

### 2ï¸âƒ£ Start the Frontend (Streamlit)

```bash
streamlit run frontend.py
```

Your browser will open automatically.

---

## ğŸ§ª Example API Request

```http
POST /chat
```

```json
{
  "model_name": "llama-3.3-70b-versatile",
  "model_provider": "Groq",
  "system_prompt": "You are a helpful AI assistant",
  "messages": ["What are the latest crypto trends?"],
  "allow_search": true
}
```

---

## ğŸ¤– Supported Models

### Groq

* âœ… `llama-3.3-70b-versatile`

### OpenAI

* âœ… `gpt-4o-mini`

### Blocked / Deprecated

* âŒ `mixtral-8x7b-32768`

---

## ğŸ›¡ï¸ Error Handling

* Provider â†” model mismatch prevention
* Graceful handling of OpenAI quota exhaustion
* Deprecated models automatically blocked
* Clear UI and API error messages

---

## ğŸ§  How It Works (High-Level)

```text
Streamlit UI
   â†“ HTTP POST
FastAPI Backend
   â†“
LangChain Agent
   â†“
Groq / OpenAI (+ Tavily Web Search)
```

---

## ğŸ“¦ Tech Stack

* Python 3.12
* FastAPI
* Streamlit
* LangChain
* Groq
* OpenAI
* Tavily
* Uvicorn

---

## âœ… Project Status

* âœ” Backend stable
* âœ” Frontend fully functional
* âœ” Groq integration working
* âš  OpenAI requires active quota

---

## ğŸ”® Future Improvements

* Chat history & memory
* Streaming responses
* Provider fallback logic
* Docker support
* Cloud deployment (AWS / Fly.io / Render)

---

## ğŸ“„ License

MIT License

---
